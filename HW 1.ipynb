{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25ed2c17",
   "metadata": {},
   "source": [
    "# MA 707 - HW 1\n",
    "\n",
    "### Dennis Wang"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a68f5f05",
   "metadata": {},
   "source": [
    "## Part A\n",
    "\n",
    "Load one of the built-in data sets from seaborn or sklearn that includes at least one categorical variable, choose a collection of feature variables (including vectorization of the categorical variable, and vectorize any other categorical feature variables you use as well) and choose a target variable (categorical or numerical, either is fine), then format your data as a supervised learning problem for these features/target and print these data frames/series. Choose a supervised learning method (you don't have to know how it works, we'll get to them next week) then do 5-fold cross validation and report the average accuracy (if your target is categorical) or some measure of error (if your target is numerical) of your method across the five trials, and also the standard deviation of the accuracies/errors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "88c5153e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bae38fc",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "titanic = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0be4626",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>sex</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>embarked</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>22.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7.2500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>C</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>female</td>\n",
       "      <td>26.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>7.9250</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>female</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>S</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>male</td>\n",
       "      <td>35.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>8.0500</td>\n",
       "      <td>S</td>\n",
       "      <td>Third</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   survived  pclass     sex   age  sibsp  parch     fare embarked  class  \\\n",
       "0         0       3    male  22.0      1      0   7.2500        S  Third   \n",
       "1         1       1  female  38.0      1      0  71.2833        C  First   \n",
       "2         1       3  female  26.0      0      0   7.9250        S  Third   \n",
       "3         1       1  female  35.0      1      0  53.1000        S  First   \n",
       "4         0       3    male  35.0      0      0   8.0500        S  Third   \n",
       "\n",
       "     who  adult_male deck  embark_town alive  alone  \n",
       "0    man        True  NaN  Southampton    no  False  \n",
       "1  woman       False    C    Cherbourg   yes  False  \n",
       "2  woman       False  NaN  Southampton   yes   True  \n",
       "3  woman       False    C  Southampton   yes  False  \n",
       "4    man        True  NaN  Southampton    no   True  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8ca6c8e",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   survived     891 non-null    int64   \n",
      " 1   pclass       891 non-null    int64   \n",
      " 2   sex          891 non-null    object  \n",
      " 3   age          714 non-null    float64 \n",
      " 4   sibsp        891 non-null    int64   \n",
      " 5   parch        891 non-null    int64   \n",
      " 6   fare         891 non-null    float64 \n",
      " 7   embarked     889 non-null    object  \n",
      " 8   class        891 non-null    category\n",
      " 9   who          891 non-null    object  \n",
      " 10  adult_male   891 non-null    bool    \n",
      " 11  deck         203 non-null    category\n",
      " 12  embark_town  889 non-null    object  \n",
      " 13  alive        891 non-null    object  \n",
      " 14  alone        891 non-null    bool    \n",
      "dtypes: bool(2), category(2), float64(2), int64(4), object(5)\n",
      "memory usage: 80.6+ KB\n"
     ]
    }
   ],
   "source": [
    "titanic.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4194dd4",
   "metadata": {},
   "source": [
    "### Dealing with missing values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28dab914",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "survived         0\n",
       "pclass           0\n",
       "sex              0\n",
       "age            177\n",
       "sibsp            0\n",
       "parch            0\n",
       "fare             0\n",
       "embarked         2\n",
       "class            0\n",
       "who              0\n",
       "adult_male       0\n",
       "deck           688\n",
       "embark_town      2\n",
       "alive            0\n",
       "alone            0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540b7713",
   "metadata": {},
   "source": [
    "### Impute Age (based on pclass) and drop NA's elsewhere"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "888971df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_age(cols):\n",
    "    Age = cols[0]\n",
    "    Pclass = cols[1]\n",
    "\n",
    "    if pd.isnull(Age):\n",
    "        if Pclass == 1:\n",
    "            return round(titanic[titanic['pclass'] == 1]['age'].dropna().mean()) # the average for 1st class\n",
    "        elif Pclass == 2:\n",
    "            return round(titanic[titanic['pclass'] == 2]['age'].dropna().mean()) # the average for 2nd class\n",
    "        else:\n",
    "            return round(titanic[titanic['pclass'] == 3]['age'].dropna().mean()) # the average for 3rd class\n",
    "    else:\n",
    "        return Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c57a30b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['age'] = titanic[['age', 'pclass']].apply(impute_age, axis = 1)\n",
    "titanic['age'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f06fcda0",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = titanic.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd58243d",
   "metadata": {},
   "source": [
    "### Vectorization of categorical variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "25fd99da",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "titanic = pd.get_dummies(titanic, columns = ['sex', 'embarked'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0285dfee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>First</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>E</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>Third</td>\n",
       "      <td>child</td>\n",
       "      <td>False</td>\n",
       "      <td>G</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    survived  pclass   age  sibsp  parch     fare  class    who  adult_male  \\\n",
       "1          1       1  38.0      1      0  71.2833  First  woman       False   \n",
       "3          1       1  35.0      1      0  53.1000  First  woman       False   \n",
       "6          0       1  54.0      0      0  51.8625  First    man        True   \n",
       "10         1       3   4.0      1      1  16.7000  Third  child       False   \n",
       "11         1       1  58.0      0      0  26.5500  First  woman       False   \n",
       "\n",
       "   deck  embark_town alive  alone  sex_male  embarked_Q  embarked_S  \n",
       "1     C    Cherbourg   yes  False         0           0           0  \n",
       "3     C  Southampton   yes  False         0           0           1  \n",
       "6     E  Southampton    no   True         1           0           1  \n",
       "10    G  Southampton   yes  False         0           0           1  \n",
       "11    C  Southampton   yes   True         0           0           1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c878a96",
   "metadata": {},
   "source": [
    "### Logistic Regression with 5-fold cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9ae9ea2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic[['pclass','age','sibsp','parch','fare', 'sex_male', 'embarked_Q', 'embarked_S']]\n",
    "y = titanic['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a5dd1b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c33dca96",
   "metadata": {},
   "outputs": [],
   "source": [
    "logmodel = LogisticRegression(max_iter = 275)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "21a53cc7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.80487805 0.8        0.75       0.675      0.775     ]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "scores = cross_val_score(logmodel, X, y, cv=5)\n",
    "print(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5447d9a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50747fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0.7609756097560976, 0.047242523620913565)\n"
     ]
    }
   ],
   "source": [
    "print((np.mean(scores), np.std(scores)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fe4b08",
   "metadata": {},
   "source": [
    "## Part B\n",
    "Now split your data into three parts: 80% training, 10% validation, 10% testing. Pick 3 different supervised learning methods (you may have to Google or browse sklearn documentation to find the names of ones you can use) and train all three on your training data (default hyperparameter values is fine, and don't worry if you don't understand the method yet) then compute their accuracy/error scores on the validation data. Pick the method that got the best score. Then combine your training data with your validation and retrain this one method on that 90% data then compute its score on the remaining 10% test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "4f9f67e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9da3c89f",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_trainval, X_test, y_trainval, y_test = train_test_split(X,y, test_size = 0.1, random_state = 42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_trainval, y_trainval, test_size = 0.1, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48ce85e",
   "metadata": {},
   "source": [
    "### Logistic Regression, KNN, and SVM\n",
    "\n",
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "4c26bfe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_model = LogisticRegression(max_iter = 300)\n",
    "log_model.fit(X_train, y_train)\n",
    "log_predictions = log_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26b3ded2",
   "metadata": {},
   "source": [
    "#### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "09e0cc87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1feb93e6",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=KNeighborsClassifier(),\n",
       "             param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34,\n",
       "       35, 36, 37, 38, 39])},\n",
       "             return_train_score=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'n_neighbors': np.arange(1,40)}\n",
    "grid = GridSearchCV(KNeighborsClassifier(), param_grid = param_grid, return_train_score = True)\n",
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b939cd73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean cross-validation score: 0.6606060606060606\n",
      "Best parameters: {'n_neighbors': 34}\n",
      "Test-set score: 0.810\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best mean cross-validation score: {grid.best_score_}\")\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Test-set score: {grid.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2c4f0388",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_model = KNeighborsClassifier(n_neighbors = grid.best_params_.get('n_neighbors'))\n",
    "knn_model.fit(X_train, y_train)\n",
    "knn_predictions = knn_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e12e82a",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "83ef40f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "86caa008",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = svm.SVC()\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_predictions = svm_model.predict(X_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ecfe8d",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a29d49c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "40c66a24",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression\n",
      "[[ 3  0]\n",
      " [ 1 14]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.75      1.00      0.86         3\n",
      "           1       1.00      0.93      0.97        15\n",
      "\n",
      "    accuracy                           0.94        18\n",
      "   macro avg       0.88      0.97      0.91        18\n",
      "weighted avg       0.96      0.94      0.95        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Logistic Regression')\n",
    "print(confusion_matrix(log_predictions, y_val))\n",
    "print('\\n')\n",
    "print(classification_report(log_predictions, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "781fe936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN\n",
      "[[ 1  0]\n",
      " [ 3 14]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.25      1.00      0.40         1\n",
      "           1       1.00      0.82      0.90        17\n",
      "\n",
      "    accuracy                           0.83        18\n",
      "   macro avg       0.62      0.91      0.65        18\n",
      "weighted avg       0.96      0.83      0.88        18\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('KNN')\n",
    "print(confusion_matrix(knn_predictions, y_val))\n",
    "print('\\n')\n",
    "print(classification_report(knn_predictions, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9e0b4559",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "[[ 0  0]\n",
      " [ 4 14]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         0\n",
      "           1       1.00      0.78      0.88        18\n",
      "\n",
      "    accuracy                           0.78        18\n",
      "   macro avg       0.50      0.39      0.44        18\n",
      "weighted avg       1.00      0.78      0.88        18\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Documents\\Anaconda\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "print('SVM')\n",
    "print(confusion_matrix(svm_predictions, y_val))\n",
    "print('\\n')\n",
    "print(classification_report(svm_predictions, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3d19fa",
   "metadata": {},
   "source": [
    "Question: I'm guessing this warning occurs because I have 0 true positives? Not sure. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb52e8e",
   "metadata": {},
   "source": [
    "### Combine training data with validation and retrain our KNN model on that 90% data then compute its score on the remaining 10% test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "fec92e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "log_test_model = LogisticRegression(max_iter = 200)\n",
    "log_test_model.fit(X_trainval, y_trainval)\n",
    "log_test_predictions = log_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a36f204e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1  0]\n",
      " [ 2 18]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.33      1.00      0.50         1\n",
      "           1       1.00      0.90      0.95        20\n",
      "\n",
      "    accuracy                           0.90        21\n",
      "   macro avg       0.67      0.95      0.72        21\n",
      "weighted avg       0.97      0.90      0.93        21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(confusion_matrix(log_test_predictions, y_test))\n",
    "print('\\n')\n",
    "print(classification_report(log_test_predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd566df5",
   "metadata": {},
   "source": [
    "### Conceptual Questions\n",
    "\n",
    "#### (a) When choosing hyperparameters (or when choosing from among different supervised learning methods) why do we split our data into three pieces (training, validation, testing) instead of the usual two (training, testing)?\n",
    "\n",
    "If we're interested in fine tuning our data, we need a validation set to test the results of modified parameters in our models that were trained on the training set. However, since we fine tuned our model on the validation set, we can't effectively test our model's performance on that same test without risking issues of overfitting. Therefore, another hold out test, the test set, is used to provide an unbiased estimate of our model's performance.\n",
    "\n",
    "#### (b) Why in the graph of accuracy as a function of model complexity does the training accuracy keep increasing but the testing accuracy goes up then back down?\n",
    "\n",
    "Because we're training our model on the training data, the training accuracy will tend to increase as the model becomes more complex because the model is \"learning\" or becoming more flexible to the training data, i.e. overfits the data. However testing accuracy will start to decrease because the complex model is starting to represent the shape of the training data too closely and has issues predicting/generalizing to new data.\n",
    "\n",
    "#### (c) Why in the graph of accuracy as a function of training set size do the training and testing accuracies get closer together as the training set size increases? Will they always \"converge\" (meet each other) as the training size goes to infinity?\n",
    "\n",
    "As you get more data points, it becomes more difficult to fit a model that captures all those data points with strong accuracy scores, and hence the training data accuracy decreases and training set size increases. With more data points however, you can build a more rigid model that better captures the trends/generalizes outside the training data, hence stronger testing/validation accuracy. There is no guarantee that the accuracies will ever converge even as training size goes to infinity.\n",
    "\n",
    "#### (d) Explain what \"stratified\" means in cross-validation.\n",
    "\n",
    "\"Stratified\" means that each of the folds produced from our KFold cross validation gets their own mini-KFold cross validation. Instead of each fold getting a segmented area dedicated to the testing set, the testing set data is instead taken from multiple parts of the fold proportional to the distribution of the target variable. This is because our data is often sorted according to the target variable so taking a single segment from the fold would likely produce testing data that does not accurately represent the distribution of our target variable.\n",
    "\n",
    "#### (e) In the above Python problem 2(b), was the accuracy/error score you got at the end on the test data better or worse than the first time you trained and computed it on the validation data? Why do you think that was case?\n",
    "\n",
    "I think it was worse because the model was fitted better to the training data and wasn't as effective in generalizing to new data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867150c1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
