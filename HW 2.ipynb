{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ff3250e",
   "metadata": {},
   "source": [
    "## HW 2\n",
    "\n",
    "Dennis Wang\n",
    "\n",
    "MA 707 - Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0afbd00",
   "metadata": {},
   "source": [
    "Find a dataset (from sklearn, seaborn, https://datasetsearch.research.google.com/, or anywhere else), choose features and a categorical target, impute missing entries if needed, split 80/20 into training and test, then fit the following models on your training data and report their accuracies on the test data: random forest, Gaussian Naive Bayes, SVM. Try this once without rescaling your data, once when normalizing it, and once when standardizing it (remember when you rescale to only do it on the feature matrix X not the target vector y, and also to fit the rescale on your training data then use it to transform the test data).\n",
    "\n",
    "NOTES: \n",
    "\n",
    "(1) You may need to convert numerical features to categorical or categorical features to numerical depending on the method---you decide what should be done in this regard and do it. \n",
    "\n",
    "(2) For methods that involve hyperparameters, just pick and try a few values by hand and use whatever gives the best test accuracy (you don't have to do a thorough grid search, though you can if you want)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fb3b2924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "08c7e269",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = sns.load_dataset('titanic')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1d5773",
   "metadata": {},
   "source": [
    "### Deal with Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b58f861b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def impute_age(cols):\n",
    "    Age = cols[0]\n",
    "    Pclass = cols[1]\n",
    "\n",
    "    if pd.isnull(Age):\n",
    "        if Pclass == 1:\n",
    "            return round(titanic[titanic['pclass'] == 1]['age'].dropna().mean()) # the average for 1st class\n",
    "        elif Pclass == 2:\n",
    "            return round(titanic[titanic['pclass'] == 2]['age'].dropna().mean()) # the average for 2nd class\n",
    "        else:\n",
    "            return round(titanic[titanic['pclass'] == 3]['age'].dropna().mean()) # the average for 3rd class\n",
    "    else:\n",
    "        return Age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "592f3d07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic['age'] = titanic[['age', 'pclass']].apply(impute_age, axis = 1)\n",
    "titanic['age'].isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "962b7866",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = titanic.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b9a9c4e",
   "metadata": {},
   "source": [
    "### Vectorization of Dummy Variabes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf5002f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic = pd.get_dummies(titanic, columns = ['sex', 'embarked'], drop_first = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "afb6da0c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>survived</th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>class</th>\n",
       "      <th>who</th>\n",
       "      <th>adult_male</th>\n",
       "      <th>deck</th>\n",
       "      <th>embark_town</th>\n",
       "      <th>alive</th>\n",
       "      <th>alone</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>38.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>71.2833</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Cherbourg</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>35.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>53.1000</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>54.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>51.8625</td>\n",
       "      <td>First</td>\n",
       "      <td>man</td>\n",
       "      <td>True</td>\n",
       "      <td>E</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>no</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>16.7000</td>\n",
       "      <td>Third</td>\n",
       "      <td>child</td>\n",
       "      <td>False</td>\n",
       "      <td>G</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>False</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>58.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>26.5500</td>\n",
       "      <td>First</td>\n",
       "      <td>woman</td>\n",
       "      <td>False</td>\n",
       "      <td>C</td>\n",
       "      <td>Southampton</td>\n",
       "      <td>yes</td>\n",
       "      <td>True</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    survived  pclass   age  sibsp  parch     fare  class    who  adult_male  \\\n",
       "1          1       1  38.0      1      0  71.2833  First  woman       False   \n",
       "3          1       1  35.0      1      0  53.1000  First  woman       False   \n",
       "6          0       1  54.0      0      0  51.8625  First    man        True   \n",
       "10         1       3   4.0      1      1  16.7000  Third  child       False   \n",
       "11         1       1  58.0      0      0  26.5500  First  woman       False   \n",
       "\n",
       "   deck  embark_town alive  alone  sex_male  embarked_Q  embarked_S  \n",
       "1     C    Cherbourg   yes  False         0           0           0  \n",
       "3     C  Southampton   yes  False         0           0           1  \n",
       "6     E  Southampton    no   True         1           0           1  \n",
       "10    G  Southampton   yes  False         0           0           1  \n",
       "11    C  Southampton   yes   True         0           0           1  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titanic.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84d73f8",
   "metadata": {},
   "source": [
    "### Split 80/20 into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7284cb33",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = titanic[['pclass','age','sibsp','parch','fare', 'sex_male', 'embarked_Q', 'embarked_S']]\n",
    "y = titanic['survived']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "70004f59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "510de468",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48bc35d9",
   "metadata": {},
   "source": [
    "### Fitting Models - No rescaling\n",
    "\n",
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b841fb3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b06e80aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators = 200)\n",
    "rfc.fit(X_train, y_train)\n",
    "rfc_predictions = rfc.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "606b278f",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "631a0724",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "00a70b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "gnb_predictions = gnb.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46c7f1a9",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9b1f9da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ec0d75d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [10, 1, 0.1, 0.01, 0.001, 0.0001, 1e-05]},\n",
       "             return_train_score=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {'C': [10, 1, 0.1, 0.01, 0.001, 0.0001, 0.00001]}\n",
    "grid = GridSearchCV(svm.SVC(), param_grid = param_grid, return_train_score = True)\n",
    "grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6c2397e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean cross-validation score: 0.66875\n",
      "Best parameters: {'C': 10}\n",
      "Test-set score: 0.732\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best mean cross-validation score: {grid.best_score_}\")\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Test-set score: {grid.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8ed95934",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = svm.SVC(C = grid.best_params_.get('C'))\n",
    "svm_model.fit(X_train, y_train)\n",
    "svm_predictions = svm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5fcf35a",
   "metadata": {},
   "source": [
    "####  Model Evaluation - No Rescaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47432fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "378be22f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest\n",
      "[[ 5  7]\n",
      " [ 2 27]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.42      0.53        12\n",
      "           1       0.79      0.93      0.86        29\n",
      "\n",
      "    accuracy                           0.78        41\n",
      "   macro avg       0.75      0.67      0.69        41\n",
      "weighted avg       0.77      0.78      0.76        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest')\n",
    "print(confusion_matrix(rfc_predictions, y_test))\n",
    "print('\\n')\n",
    "print(classification_report(rfc_predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6727b69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes\n",
      "[[ 6  9]\n",
      " [ 1 25]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.40      0.55        15\n",
      "           1       0.74      0.96      0.83        26\n",
      "\n",
      "    accuracy                           0.76        41\n",
      "   macro avg       0.80      0.68      0.69        41\n",
      "weighted avg       0.78      0.76      0.73        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Gaussian Naive Bayes')\n",
    "print(confusion_matrix(gnb_predictions, y_test))\n",
    "print('\\n')\n",
    "print(classification_report(gnb_predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7f174c02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "[[ 0  4]\n",
      " [ 7 30]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         4\n",
      "           1       0.88      0.81      0.85        37\n",
      "\n",
      "    accuracy                           0.73        41\n",
      "   macro avg       0.44      0.41      0.42        41\n",
      "weighted avg       0.80      0.73      0.76        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('SVM')\n",
    "print(confusion_matrix(svm_predictions, y_test))\n",
    "print('\\n')\n",
    "print(classification_report(svm_predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "accd8478",
   "metadata": {},
   "source": [
    "### Fitting models - Normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "04fa6862",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e072711d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.009973</td>\n",
       "      <td>0.558513</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.009973</td>\n",
       "      <td>0.829376</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.006346</td>\n",
       "      <td>0.368053</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.929783</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.089013</td>\n",
       "      <td>0.741773</td>\n",
       "      <td>0.029671</td>\n",
       "      <td>0.029671</td>\n",
       "      <td>0.663392</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.008504</td>\n",
       "      <td>0.425177</td>\n",
       "      <td>0.008504</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.904990</td>\n",
       "      <td>0.008504</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.014178</td>\n",
       "      <td>0.666352</td>\n",
       "      <td>0.014178</td>\n",
       "      <td>0.014178</td>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.014178</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pclass       age     sibsp     parch      fare  sex_male  embarked_Q  \\\n",
       "0  0.009973  0.558513  0.000000  0.009973  0.829376  0.000000         0.0   \n",
       "1  0.006346  0.368053  0.000000  0.000000  0.929783  0.000000         0.0   \n",
       "2  0.089013  0.741773  0.029671  0.029671  0.663392  0.000000         0.0   \n",
       "3  0.008504  0.425177  0.008504  0.000000  0.904990  0.008504         0.0   \n",
       "4  0.014178  0.666352  0.014178  0.014178  0.745098  0.000000         0.0   \n",
       "\n",
       "   embarked_S  \n",
       "0    0.000000  \n",
       "1    0.000000  \n",
       "2    0.000000  \n",
       "3    0.000000  \n",
       "4    0.014178  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalizer = Normalizer()\n",
    "normalizer.fit(X_train) # fit to data, not the target class\n",
    "norm_features = normalizer.transform(X_train)\n",
    "X_test_norm = normalizer.transform(X_test)\n",
    "\n",
    "X_tr_norm = pd.DataFrame(norm_features, columns = X_train.columns)\n",
    "X_tr_norm.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d2379a",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "52113d55",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_tr_norm, y_train)\n",
    "rfc_norm_predictions = rfc.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f7240f9",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "35ccadd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X_tr_norm, y_train)\n",
    "gnb_norm_predictions = gnb.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf9bcf04",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f496eb73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [10, 1, 0.1, 0.01, 0.001, 0.0001, 1e-05]},\n",
       "             return_train_score=True)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_tr_norm, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "021ce049",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean cross-validation score: 0.6625\n",
      "Best parameters: {'C': 10}\n",
      "Test-set score: 0.171\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best mean cross-validation score: {grid.best_score_}\")\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Test-set score: {grid.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a3788336",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = svm.SVC(C = grid.best_params_.get('C'))\n",
    "svm_model.fit(X_tr_norm, y_train)\n",
    "svm_norm_predictions = svm_model.predict(X_test_norm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0e0ad87",
   "metadata": {},
   "source": [
    "#### Model Evaluation - Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8a9381d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Normalized\n",
      "[[ 4  6]\n",
      " [ 3 28]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.57      0.40      0.47        10\n",
      "           1       0.82      0.90      0.86        31\n",
      "\n",
      "    accuracy                           0.78        41\n",
      "   macro avg       0.70      0.65      0.67        41\n",
      "weighted avg       0.76      0.78      0.77        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest - Normalized')\n",
    "print(confusion_matrix(rfc_norm_predictions, y_test))\n",
    "print('\\n')\n",
    "print(classification_report(rfc_norm_predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ec0218f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes - Normalized\n",
      "[[ 1  3]\n",
      " [ 6 31]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.14      0.25      0.18         4\n",
      "           1       0.91      0.84      0.87        37\n",
      "\n",
      "    accuracy                           0.78        41\n",
      "   macro avg       0.53      0.54      0.53        41\n",
      "weighted avg       0.84      0.78      0.81        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Gaussian Naive Bayes - Normalized')\n",
    "print(confusion_matrix(gnb_norm_predictions, y_test))\n",
    "print('\\n')\n",
    "print(classification_report(gnb_norm_predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "ebfe6ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - Normalized\n",
      "[[ 0  1]\n",
      " [ 7 33]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.00      0.00      0.00         1\n",
      "           1       0.97      0.82      0.89        40\n",
      "\n",
      "    accuracy                           0.80        41\n",
      "   macro avg       0.49      0.41      0.45        41\n",
      "weighted avg       0.95      0.80      0.87        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('SVM - Normalized')\n",
    "print(confusion_matrix(svm_norm_predictions, y_test))\n",
    "print('\\n')\n",
    "print(classification_report(svm_norm_predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ec4bbd2",
   "metadata": {},
   "source": [
    "### Fitting Models - Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4de7a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a205a9fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pclass</th>\n",
       "      <th>age</th>\n",
       "      <th>sibsp</th>\n",
       "      <th>parch</th>\n",
       "      <th>fare</th>\n",
       "      <th>sex_male</th>\n",
       "      <th>embarked_Q</th>\n",
       "      <th>embarked_S</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.412751</td>\n",
       "      <td>1.303612</td>\n",
       "      <td>-0.660979</td>\n",
       "      <td>0.797861</td>\n",
       "      <td>0.147497</td>\n",
       "      <td>-1.119608</td>\n",
       "      <td>-0.138233</td>\n",
       "      <td>-1.308382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-0.412751</td>\n",
       "      <td>1.433991</td>\n",
       "      <td>-0.660979</td>\n",
       "      <td>-0.589723</td>\n",
       "      <td>0.977495</td>\n",
       "      <td>-1.119608</td>\n",
       "      <td>-0.138233</td>\n",
       "      <td>-1.308382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3.063047</td>\n",
       "      <td>-0.717252</td>\n",
       "      <td>1.101632</td>\n",
       "      <td>0.797861</td>\n",
       "      <td>-0.648935</td>\n",
       "      <td>-1.119608</td>\n",
       "      <td>-0.138233</td>\n",
       "      <td>-1.308382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.412751</td>\n",
       "      <td>0.912477</td>\n",
       "      <td>1.101632</td>\n",
       "      <td>-0.589723</td>\n",
       "      <td>0.452272</td>\n",
       "      <td>0.893170</td>\n",
       "      <td>-0.138233</td>\n",
       "      <td>-1.308382</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.412751</td>\n",
       "      <td>0.716910</td>\n",
       "      <td>1.101632</td>\n",
       "      <td>0.797861</td>\n",
       "      <td>-0.253393</td>\n",
       "      <td>-1.119608</td>\n",
       "      <td>-0.138233</td>\n",
       "      <td>0.764303</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     pclass       age     sibsp     parch      fare  sex_male  embarked_Q  \\\n",
       "0 -0.412751  1.303612 -0.660979  0.797861  0.147497 -1.119608   -0.138233   \n",
       "1 -0.412751  1.433991 -0.660979 -0.589723  0.977495 -1.119608   -0.138233   \n",
       "2  3.063047 -0.717252  1.101632  0.797861 -0.648935 -1.119608   -0.138233   \n",
       "3 -0.412751  0.912477  1.101632 -0.589723  0.452272  0.893170   -0.138233   \n",
       "4 -0.412751  0.716910  1.101632  0.797861 -0.253393 -1.119608   -0.138233   \n",
       "\n",
       "   embarked_S  \n",
       "0   -1.308382  \n",
       "1   -1.308382  \n",
       "2   -1.308382  \n",
       "3   -1.308382  \n",
       "4    0.764303  "
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train) # fit to data, not the target class\n",
    "scaled_features = scaler.transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "X_tr_scaled = pd.DataFrame(scaled_features, columns = X_train.columns)\n",
    "X_tr_scaled.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac53f60",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "146817c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc.fit(X_tr_scaled, y_train)\n",
    "rfc_scaled_predictions = rfc.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fdacbb",
   "metadata": {},
   "source": [
    "#### Gaussian Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f547ff9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.fit(X_tr_scaled, y_train)\n",
    "gnb_scaled_predictions = gnb.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049bac50",
   "metadata": {},
   "source": [
    "#### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ec0bb13d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=SVC(),\n",
       "             param_grid={'C': [10, 1, 0.1, 0.01, 0.001, 0.0001, 1e-05]},\n",
       "             return_train_score=True)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_tr_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a8792268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best mean cross-validation score: 0.725\n",
      "Best parameters: {'C': 1}\n",
      "Test-set score: 0.829\n"
     ]
    }
   ],
   "source": [
    "print(f\"Best mean cross-validation score: {grid.best_score_}\")\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Test-set score: {grid.score(X_test, y_test):.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b11035ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model = svm.SVC(C = grid.best_params_.get('C'))\n",
    "svm_model.fit(X_tr_scaled, y_train)\n",
    "svm_scaled_predictions = svm_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47c5587",
   "metadata": {},
   "source": [
    "#### Model Evaluation - Scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "48bead5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest - Scaled\n",
      "[[ 5  8]\n",
      " [ 2 26]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.38      0.50        13\n",
      "           1       0.76      0.93      0.84        28\n",
      "\n",
      "    accuracy                           0.76        41\n",
      "   macro avg       0.74      0.66      0.67        41\n",
      "weighted avg       0.75      0.76      0.73        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Random Forest - Scaled')\n",
    "print(confusion_matrix(rfc_scaled_predictions, y_test))\n",
    "print('\\n')\n",
    "print(classification_report(rfc_scaled_predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ac9bef57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Gaussian Naive Bayes - Scaled\n",
      "[[ 6  9]\n",
      " [ 1 25]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.40      0.55        15\n",
      "           1       0.74      0.96      0.83        26\n",
      "\n",
      "    accuracy                           0.76        41\n",
      "   macro avg       0.80      0.68      0.69        41\n",
      "weighted avg       0.78      0.76      0.73        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('Gaussian Naive Bayes - Scaled')\n",
    "print(confusion_matrix(gnb_scaled_predictions, y_test))\n",
    "print('\\n')\n",
    "print(classification_report(gnb_scaled_predictions, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5b1a660e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM - Scaled\n",
      "[[ 6  7]\n",
      " [ 1 27]]\n",
      "\n",
      "\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.46      0.60        13\n",
      "           1       0.79      0.96      0.87        28\n",
      "\n",
      "    accuracy                           0.80        41\n",
      "   macro avg       0.83      0.71      0.74        41\n",
      "weighted avg       0.81      0.80      0.79        41\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('SVM - Scaled')\n",
    "print(confusion_matrix(svm_scaled_predictions, y_test))\n",
    "print('\\n')\n",
    "print(classification_report(svm_scaled_predictions, y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d120b30",
   "metadata": {},
   "source": [
    "### Conceptual Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d16a23",
   "metadata": {},
   "source": [
    "#### From Sep 15 lecture:\n",
    "\n",
    ">Describe in words the \"backwards elimination\" process for feature selection\n",
    "\n",
    "When selecting which features to include in your model, backwards eliminations involves starting with a model with all the features included, and then eliminating the feature that leads to the the model with the best accuracy score. Keep eliminating features and creating new models until you no longer get better accuracy scores.\n",
    "\n",
    ">Explain why some categorical variables need to be vectorized (one-hot encoding) whereas others can be label encoded (just assigned a different number for each value)\n",
    "\n",
    "If there is an implicit order to the categorical variable, e.g. the darkness level of a crab shell sorted as Dark, Medium Dark, Medium, etc., then it can be assigned a different number for each value and the number would correspond to that implicit order. However, if there is no implicit order to the categorical variable, such as the state that a package is shipped to, then there is no way to judge whether one state is greater than another (e.g. NJ > MA), and thus the variable will need to be vectorized.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1fd9c9",
   "metadata": {},
   "source": [
    "#### k-NN:\n",
    "\n",
    ">How does k-NN label new data points based on the training data points (both for classification and for regression)?\n",
    "\n",
    ">What effect does the hyperparmater k have?\n",
    "\n",
    "The hyperparameter k is how many of the closest k points to our test data point will be considered in our judgement. Smaller k-values make the model more complex and sensitive to the local region of our test value, but increase the risk of overfitting. Therefore, if we think the local structure of our data points are important, then a smaller K is better. Bigger K-values are more representative of our overall dataset but less sensitive to local regions of the data, which may also be bad.\n",
    "\n",
    "In classification, we simply take the majority label of our k neighbors and select that as the prediction for our test data point. In regression, we take the mean of all the k neighbors and choose that as our prediction. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afcdc1b7",
   "metadata": {},
   "source": [
    "#### Naive Bayes:\n",
    "\n",
    ">How does Bayes formula help with classification if we've already estimated the probability distribution for each class?\n",
    "\n",
    "Say we we have two classes `Men` and `Women`, and we are trying to predict for a certain `Disease`: P(Disease | Men) and P(Disease | Women). We cannot simply say that because of the probability of `Disease` is higher for `Men` than it is for `Women`, that our label for certain is going to be `Men`. This is because we're assuming that the dataset is balanced between `Men` and `Women` labels when it could be heavily biased towards one or the other. Therefore, having a probability of `Disease` that is higher for `Men` than it is for `Women` doesn't mean much if `Men` make up 1% of the dataset and `Women` 99%. \n",
    "\n",
    "A much more useful metric if instead the flip of our probability: P(Men | Disease) and P(Women | Disease). This would help us incorporate the fact that the dataset may be imbalanced and properly turns these class distributions into a classifier.\n",
    "\n",
    ">Explain how the class distributions are estimated in Gaussian Naive Bayes\n",
    "\n",
    "In Gaussian Naive Bayes, all of our variables/features are numerical and we assume the distribution of each class to be Gaussian with no covariance. We then take the mean and variance of each variable in each label(where each Gaussian is centered), thereby granting us the entire class distribution.\n",
    "\n",
    ">Very roughly, what are Gaussian Mixture Model (GMM) and Kernel Density Estimator (KDE)?\n",
    "\n",
    "If we're assuming each class to be Gaussian but also have some covariance (e.g. you think the predictors might be related), then we use the Gaussian Mixture Model. GMM allows us to fit the data in different ways by mixing different Gaussian models and see which results in the best scores.\n",
    "\n",
    "Kernel Density Estimator (KDE) works by creating the distribution from the data. We not only assume that each class as Gaussian, but replace each point with a Gaussian distribution. Essentially we build a Gaussian distribution around each data point, and when these Gaussians overlap, we can add up all the points of the Gaussians and it'll build the distribution.\n",
    "\n",
    ">Describe Naive Bayes classification when all predictors are discrete (this is the frequency-based version) including what the \"Naive\" part of it means)\n",
    "\n",
    "In a Naive Bayes Classification, the predictors are discrete. You can view your target conditional probability as the fraction of your class training points that have those particular values for the predictors. For example, if you're looking for the probability of `Cat`, `Dog`, and `Mouse` keywords appearing in an email to differentiate spam vs. not spam, P( (1,1,1) | not spam), you would go through all the emails in your dataset that have exactly one mention of each keyword, count those emails, and divide by the total number of emails marked as not spam. \n",
    "\n",
    "The problem is that there's a good chance you may not have a vector that exactly matches yours, e.g. a vector that has exactly 2 mentions of `Cat`, 5 mentions of `Dog`, and 7 mentions of `Mouse` may be not appear. In practice we often don't have any training points with these exact predictor values.\n",
    "\n",
    "As such, we will pretend that all of our predictors are conditionally independent. This is where the \"Naive\" part comes from: It's not true but it's close enough to being true where it's still useful. From our example, we're pretending that our words are not related. We're claiming that the number of times the word `Cat` appears in an email has nothing to do with the appearances of `Dog` or `Mouse` in that same email, which is very likely not true in real, but we're doing so regardless.\n",
    "\n",
    "This works because in real life, where we're assuming dependence between our variables (or keywor apperances in an email), seeing one word will push us towards a single label, and another keyword may also push us towards that label. However, because these keywords are conditionally dependent, this \"push effect\" is not as strong as if they were conditionally independently pushing towards that label. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0986bdb6",
   "metadata": {},
   "source": [
    "#### Decision trees and random forest:\n",
    "\n",
    ">Explain the basic idea of a decision tree and what kind of decision boundary it produces\n",
    "\n",
    "A decision tree works by continuously splitting your along the axis of each prredictor. Each split represents a decision point in the tree, and creates a decision boundary.\n",
    "\n",
    ">How are random forests related to decision trees?\n",
    "\n",
    "Random forests are an ensemble method which means that it trains a bunch of different decision trees based on random sampling (with replacement) of our training data. From the results of each tree, we average out each prediction if its regression, or take the majority label if we're doing classification. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23aff0c6",
   "metadata": {},
   "source": [
    "#### SVM:\n",
    "\n",
    ">Explain the basic idea and how the cost parameter C is used and what it does (no need to explain anything about kernels or radial basis functions)\n",
    "\n",
    "The goal of SVM is to choose the best linear decision boundary to separate our data points. This works by choosing the line that maximizes the distance from the closest data points (known as support points or vectors). The cost parameter C is a parameter of the SVM function. It determines the cost of data points that enter deep into our margin or cross our line. IF C is large, thyen we don't have many points in our margin or crossing the line at all as we can't afford to let a lot of points cross the margin, but the expense of this is that our margin then becomes very thin as a result and may lead to a model that can't generalize to new data very well. A low C can afford to let a lot of points cross the margin and as a result results in a larger margin. The problem with this is that you may not properly capture the trends/shape of the data with your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600fc162",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
